
# __PyTorch__

We've integrated `pytorch` into benchmarking suite using similar to TensorFlow approach. We've written [pytorch_benchmarks](https://github.com/HewlettPackard/dlcookbook-dlbs/tree/master/python/pytorch_benchmarks) python projects that exposes similar command line API as tf_cnn_benchmarks. This is a quick overview of this implementation.

## Standalone run
The project itself is located in [pytorch_benchmarks](https://github.com/HewlettPackard/dlcookbook-dlbs/tree/master/python/pytorch_benchmarks) folder. The [benchmarks.py](https://github.com/HewlettPackard/dlcookbook-dlbs/blob/master/python/pytorch_benchmarks/benchmarks.py) file is the entry point. Following command line parameters are supported (some of parameter descriptions are taken from pytorch project on github):
1.  `--model` (type=str) A model to benchmark ('alexnet', 'googlenet' ...).
2.  `--forward_only` (type=bool) Benchmark inference (if true) else benchmark training.
3.  `--batch_size` (type=int) Per device batch size.
4.  `--num_batches` (type=int) Number of benchmark iterations.
5.  `--num_warmup_batches` (type=int) Number of warmup iterations
6.  `--num_gpus` (type=int) Number of gpus to use. Use CUDA_VISIBLE_DEVICES to select those devices.
7.  `--device` (type=str) Comptue device, 'cpu' or 'gpu'.
8.  `--dtype` (choices=['float', 'float32', 'float16']) Precision of data variables: float(same as float32), float32 or float16.
9.  `--data_dir` (type=str) Path to a dataset.
10. `--cudnn_benchmark` (type=bool) Uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. If this is set to false, uses some in-built heuristics that might not always be fastest. By default cudnn_benchmark is set to TRUE. Setting to true will improve performance, at the expense of using more memory. The input shape should be the same for each batch, otherwise autotune will re-run for each batch, causing a huge slow-down. More details are [here](https://github.com/soumith/cudnn.torch#modes).
11. `--cudnn_fastest` (type=bool) Enables a fast mode for the Convolution modules - simply picks the fastest convolution algorithm, rather than tuning for workspace size. By default, cudnn.fastest is set to false. You should set to true if memory is not an issue, and you want the fastest performance. More details are [here](https://github.com/soumith/cudnn.torch#modes).
12. `--data_shuffle` (type=bool) Enable/disable shuffling for both real/synthetic datasets.
13. `--num_loader_threads` (type=int) Number of dataset loader threads.
14. `--data_backend` (choices=['caffe_lmdb', 'image_folder']) In case of real data, specifies its storage backend ('caffe_lmdb' or 'image_folder'). The following datasets are supported: (1) Caffe's LMDB datasets. DLBS can use LMDB files generated by Caffe to run benchmarks and (2) datasets of raw images. It is the ImageFolder dataset from torchvision project.

## Models
The class `ModelFactory` in [model_factory.py](https://github.com/HewlettPackard/dlcookbook-dlbs/blob/master/python/pytorch_benchmarks/model_factory.py) is responsible for creating neural networks. Complete list of supported models can be found on the [model](/models/models.md?id=supported-models) page.

## Adding new model
To add new model, several steps need to be performed:
1. Add a new file in [models](https://github.com/HewlettPackard/dlcookbook-dlbs/tree/master/python/pytorch_benchmarks/models) folder. Create a class and inherit it from [pytorch_benchmarks.models.model.Model](https://github.com/HewlettPackard/dlcookbook-dlbs/blob/master/python/pytorch_benchmarks/models/model.py).
2. Study any implementation of the existing model. The [DeepMNIST](https://github.com/HewlettPackard/dlcookbook-dlbs/blob/master/python/pytorch_benchmarks/models/deep_mnist.py) model is probably the simplest one.
    1. Each class defining a neural network is basically a pytoch [module](http://pytorch.org/docs/master/nn.html#torch.nn.Module). So, you need to follow rules when developing models. I usually define operators or entire models in constructor and then returning a graph in `forward` method.
    2. Each class must override `forward` method, for instance:
       ```python
       def forward(self, x):
           return self.model(x)
       ```
       where `self.model` is a module defined in constructor, that can be, for instance, a sequential module.


## Commonly used configuration parameters
#### __pytorch.cudnn_benchmark__

* __default value__ `True`
* __description__ Uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. If this is set to false, uses some in-built heuristics that might not always be fastest. By default cudnn_benchmark is set to TRUE. Setting to true will improve performance, at the expense of using more memory. The input shape should be the same for each batch, otherwise autotune will re-run for each batch, causing a huge slow-down. More details are here: https://github.com/soumith/cudnn.torch#modes

#### __pytorch.cudnn_fastest__

* __default value__ `False`
* __description__ Enables a fast mode for the Convolution modules - simply picks the fastest convolution algorithm, rather than tuning for workspace size. By default, cudnn.fastest is set to false. You should set to true if memory is not an issue, and you want the fastest performance. More details are here: https://github.com/soumith/cudnn.torch#modes

#### __pytorch.docker_image__

* __default value__ `"hpe/pytorch:cuda9-cudnn7"`
* __description__ The name of a docker image to use for PyTorch if containerized benchmark is requested.


## Other parameters
#### __pytorch.args__

* __default value__ `[u'--model=${exp.model}', u"--forward_only=$('true' if '${exp.phase}'=='inference' else 'false')$", u'--batch_size=${exp.replica_batch}', u'--num_batches=${exp.num_batches}', u'--num_warmup_batches=${exp.num_warmup_batches}', u'--num_gpus=${exp.num_local_gpus}', u'--device=${exp.device_type}', u"$('' if not '${pytorch.data_dir}' else '--data_dir=${pytorch.data_dir}' if ${exp.docker} is False else '--data_dir=/workspace/data')$", u'--data_backend=${pytorch.data_backend}', u"--data_shuffle=$('true' if ${pytorch.data_shuffle} else 'false')$", u'--num_loader_threads=${pytorch.num_loader_threads}', u'--dtype=${exp.dtype}', u"--cudnn_benchmark=$('true' if ${pytorch.cudnn_benchmark} else 'false')$", u"--cudnn_fastest=$('true' if ${pytorch.cudnn_fastest} else 'false')$"]`
* __description__ Command line arguments that launcher will pass to a pytorch_benchmarks script.

#### __pytorch.bench_path__

* __default value__ `"$('${DLBS_ROOT}/python' if not ${exp.docker} else '/workspace')$"`
* __description__ Python path to where pytorch_benchmarks project is located. Depends on bare metal/docker benchmarks.

#### __pytorch.data_backend__

* __default value__ `"caffe_lmdb"`
* __description__ In case of real data, specifies its storage backend \('caffe_lmdb' or 'image_folder'\). The following datasets are supported:   1. Caffe's LMDB datasets. DLBS can use LMDB files generated by Caffe to run benchmarks.   2. Datasets of raw images. It is the ImageFolder dataset from torchvision project.

#### __pytorch.data_dir__

* __default value__ `""`
* __description__ A data directory if real data should be used. If empty, synthetic data is used \(no data ingestion pipeline\).

#### __pytorch.data_shuffle__

* __default value__ `False`
* __description__ Enable/disable shuffling for both real and synthetic datasets.

#### __pytorch.docker_args__

* __default value__ `[u'-i', u'--security-opt seccomp=unconfined', u'--pid=host', u'--ipc=host', u'--volume=${DLBS_ROOT}/python/pytorch_benchmarks:/workspace/pytorch_benchmarks', u"$('--volume=${runtime.cuda_cache}:/workspace/cuda_cache' if '${runtime.cuda_cache}' else '')$", u"$('--volume=${pytorch.data_dir}:/workspace/data' if '${pytorch.data_dir}' else '')$", u"$('--volume=${monitor.pid_folder}:/workspace/tmp' if ${monitor.frequency} > 0 else '')$", u'${exp.docker_args}', u'${exp.docker_image}']`
* __description__ In case if containerized benchmarks, this are the docker parameters.

#### __pytorch.env__

* __default value__ `""`
* __description__ An environment \(a bunch of export directives\) for the PyTorch framework.

#### __pytorch.host_libpath__

* __default value__ `""`
* __description__ Basically, it's a LD_LIBRARY_PATH for PyTorch in case of a bare metal run.

#### __pytorch.host_python_path__

* __default value__ `"${HOME}/projects/pytorch/python"`
* __description__ Path to a PyTorch's python folder in case of a bare metal run.

#### __pytorch.launcher__

* __default value__ `"${DLBS_ROOT}/scripts/launchers/pytorch.sh"`
* __description__ Path to script that launches PyTorch benchmarks.

#### __pytorch.num_loader_threads__

* __default value__ `4`
* __description__ Number of worker threads to be used by data loader \(for synthetic and real datasets\).
