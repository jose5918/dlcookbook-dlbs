{
  "parameters": {
    "pytorch.launcher": {
      "val": "${DLBS_ROOT}/scripts/launchers/pytorch.sh",
      "type": "str",
      "desc": "Path to script that launches PyTorch benchmarks."
    },
    "pytorch.env": {
      "val": "",
      "type": "str",
      "desc": "An environment (a bunch of export directives) for the PyTorch framework."
    },
    "pytorch.bench_path": {
      "val": "$('${DLBS_ROOT}/python' if not ${exp.docker} else '/workspace')$",
      "type": "str",
      "desc": "Python path to where pytorch_benchmarks project is located. Depends on bare metal/docker benchmarks."
    },
    "pytorch.data_dir": {
      "val": "",
      "type": "str",
      "desc": "A data directory if real data should be used. If empty, synthetic data is used (no data ingestion pipeline)."
    },
    "pytorch.data_backend": {
      "val": "caffe_lmdb",
      "type": "str",
      "val_domain":["caffe_lmdb", "image_folder"],
      "desc": [
        "In case of real data, specifies its storage backend ('caffe_lmdb' or 'image_folder'). The following datasets are",
        "supported:",
        "  1. Caffe's LMDB datasets. DLBS can use LMDB files generated by Caffe to run benchmarks.",
        "  2. Datasets of raw images. It is the ImageFolder dataset from torchvision project."
      ]
    },
    "pytorch.cudnn_benchmark": {
      "val": true,
      "type": "bool",
      "desc": [
        "Uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. If this is set to false,",
        "uses some in-built heuristics that might not always be fastest. By default cudnn_benchmark is set to TRUE.",
        "Setting to true will improve performance, at the expense of using more memory. The input shape should be",
        "the same for each batch, otherwise autotune will re-run for each batch, causing a huge slow-down.",
        "More details are here: https://github.com/soumith/cudnn.torch#modes"
      ]
    },
    "pytorch.cudnn_fastest": {
      "val": false,
      "type": "bool",
      "desc":[
        "Enables a fast mode for the Convolution modules - simply picks the fastest convolution algorithm,",
        "rather than tuning for workspace size. By default, cudnn.fastest is set to false. You should set",
        "to true if memory is not an issue, and you want the fastest performance. More details are here:",
        "https://github.com/soumith/cudnn.torch#modes"
      ]
    },
    "pytorch.data_shuffle": {
      "val": false,
      "type": "bool",
      "desc": "Enable/disable shuffling for both real and synthetic datasets."
    },
    "pytorch.num_loader_threads": {
      "val": 4,
      "type": "int",
      "desc": "Number of worker threads to be used by data loader (for synthetic and real datasets)."
    },
    "pytorch.data_loader_only": {
      "val": false,
      "type": "bool",
      "desc": "If true, benchmark only data ingestion part of the workload (data loader)."
    },
    "pytorch.args": {
      "val": [
        "--model=${exp.model}",
        "--forward_only=$('true' if '${exp.phase}'=='inference' else 'false')$",
        "--batch_size=${exp.replica_batch}",
        "--num_batches=${exp.num_batches}",
        "--num_warmup_batches=${exp.num_warmup_batches}",
        "--num_gpus=${exp.num_local_gpus}",
        "--device=${exp.device_type}",
        "$('' if not '${exp.data_dir}' else '--data_dir=${exp.data_dir}' if ${exp.docker} is False else '--data_dir=/workspace/data')$",
        "--data_backend=${pytorch.data_backend}",
        "--data_shuffle=$('true' if ${pytorch.data_shuffle} else 'false')$",
        "--num_loader_threads=${pytorch.num_loader_threads}",
        "--dtype=${exp.dtype}",
        "--cudnn_benchmark=$('true' if ${pytorch.cudnn_benchmark} else 'false')$",
        "--cudnn_fastest=$('true' if ${pytorch.cudnn_fastest} else 'false')$",
        "--data_loader_only=$('true' if ${pytorch.data_loader_only} is True else 'false')$"
      ],
      "type": "str",
      "desc": "Command line arguments that launcher will pass to a pytorch_benchmarks script."
    },
    "pytorch.docker_image": {
      "val": "hpe/pytorch:cuda9-cudnn7",
      "type": "str",
      "desc": "The name of a docker image to use for PyTorch if containerized benchmark is requested."
    },
    "pytorch.docker_args": {
      "val": [
        "-i",
        "--security-opt seccomp=unconfined",
        "--pid=host",
        "--ipc=host",
        "--volume=${DLBS_ROOT}/python/pytorch_benchmarks:/workspace/pytorch_benchmarks",
        "$('--volume=${runtime.cuda_cache}:/workspace/cuda_cache' if '${runtime.cuda_cache}' else '')$",
        "$('--volume=${exp.data_dir}:/workspace/data' if '${exp.data_dir}' else '')$",
        "$('--volume=${monitor.pid_folder}:/workspace/tmp' if ${monitor.frequency} > 0 else '')$",
        "${exp.docker_args}",
        "${exp.docker_image}"
      ],
      "type": "str",
      "desc": "In case if containerized benchmarks, this are the docker parameters."
    },
    "pytorch.host_python_path": {
      "val": "${HOME}/projects/pytorch/python",
      "type": "str",
      "desc": "Path to a PyTorch's python folder in case of a bare metal run."
    },
    "pytorch.host_libpath": {
      "val": "",
      "type": "str",
      "desc": "Basically, it's a LD_LIBRARY_PATH for PyTorch in case of a bare metal run."
    }
  },
  "extensions": [
    {
      "condition":{ "exp.framework": "pytorch", "exp.docker": true },
      "parameters": { "pytorch.env": [
        "PYTHONPATH=$('${pytorch.bench_path}:\\$PYTHONPATH'.strip(' \t:'))$",
        "${runtime.EXPORT_CUDA_CACHE_PATH}",
        "${runtime.EXPORT_CUDA_VISIBLE_DEVICES}"
      ]}
    },
    {
      "condition":{ "exp.framework": "pytorch", "exp.docker": false },
      "parameters": { "pytorch.env": [
        "LD_LIBRARY_PATH=$('${pytorch.host_libpath}:\\$LD_LIBRARY_PATH'.strip(' \t:'))$",
        "PYTHONPATH=$('${pytorch.host_python_path}:${pytorch.bench_path}:\\$PYTHONPATH'.strip(' \t:'))$",
        "${runtime.EXPORT_CUDA_CACHE_PATH}",
        "${runtime.EXPORT_CUDA_VISIBLE_DEVICES}"
      ]}
    }
  ]
}
